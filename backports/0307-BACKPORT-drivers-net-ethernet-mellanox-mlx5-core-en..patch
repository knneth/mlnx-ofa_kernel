From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en.h

Change-Id: I5c7b31ea9c23e49a63a181c269a0385fab2e7a9d
---
 drivers/net/ethernet/mellanox/mlx5/core/en.h | 270 +++++++++++++++++--
 1 file changed, 254 insertions(+), 16 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -32,9 +32,15 @@
 #ifndef __MLX5_EN_H__
 #define __MLX5_EN_H__
 
+#ifdef HAVE_XDP_SUPPORT
+#include <linux/bpf.h>
+#endif
 #include <linux/if_vlan.h>
 #include <linux/etherdevice.h>
 #include <linux/timecounter.h>
+#ifdef HAVE_BASECODE_EXTRAS
+#include <linux/clocksource.h>
+#endif
 #include <linux/net_tstamp.h>
 #include <linux/crash_dump.h>
 #include <linux/mlx5/driver.h>
@@ -45,7 +51,12 @@
 #include <linux/mlx5/transobj.h>
 #include <linux/mlx5/fs.h>
 #include <linux/rhashtable.h>
+#ifdef HAVE_BASECODE_EXTRAS
+#include <linux/ethtool.h>
+#endif
+#ifdef HAVE_UDP_TUNNEL_NIC_INFO
 #include <net/udp_tunnel.h>
+#endif
 #include <net/switchdev.h>
 #include <net/xdp.h>
 #include <net/pkt_cls.h>
@@ -60,12 +71,22 @@
 #include "lib/hv_vhca.h"
 #include "lib/clock.h"
 #include "en/rx_res.h"
+#include <net/ip.h>
+
+#ifndef HAVE_DEVLINK_HEALTH_REPORT_SUPPORT
+/* The intention is to pass NULL for backports of old kernels */
+struct devlink_health_reporter {};
+#endif /* HAVE_DEVLINK_HEALTH_REPORT_SUPPORT */
 #include "en/selq.h"
 #include "lib/sd.h"
 
 extern const struct net_device_ops mlx5e_netdev_ops;
 struct page_pool;
 
+#ifndef HAVE_XSK_BUFF_ALLOC_BATCH
+#define MLX5_ALIGNED_MTTS_OCTW(mtts)	((mtts) / 2)
+#endif
+
 #define MLX5E_LRO_TIMEOUT_ARR_SIZE 4
 
 #define MLX5E_METADATA_ETHER_TYPE (0x8CE4)
@@ -258,13 +279,19 @@ enum mlx5e_priv_flag {
 	MLX5E_PFLAG_RX_CQE_BASED_MODER,
 	MLX5E_PFLAG_TX_CQE_BASED_MODER,
 	MLX5E_PFLAG_RX_CQE_COMPRESS,
+#ifdef HAVE_BASECODE_EXTRAS
+	MLX5E_PFLAG_TX_CQE_COMPRESS,
+#endif
 	MLX5E_PFLAG_RX_STRIDING_RQ,
 	MLX5E_PFLAG_RX_NO_CSUM_COMPLETE,
+#ifdef HAVE_XDP_SUPPORT
 	MLX5E_PFLAG_XDP_TX_MPWQE,
+#endif
 	MLX5E_PFLAG_SKB_TX_MPWQE,
 	MLX5E_PFLAG_TX_PORT_TS,
 	MLX5E_PFLAG_DROPLESS_RQ,
 	MLX5E_PFLAG_PER_CH_STATS,
+	/* OFED-specific private flags */
 	MLX5E_PFLAG_TX_XDP_CSUM,
 	MLX5E_NUM_PFLAGS, /* Keep last */
 };
@@ -298,6 +325,9 @@ struct mlx5e_params {
 	u8  log_sq_size;
 	u8  rq_wq_type;
 	u8  log_rq_mtu_frames;
+#if !defined(HAVE_PAGE_POOL_DEFRAG_PAGE) && !defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
+	u8  log_rx_page_cache_mult;
+#endif
 	u16 num_channels;
 	struct {
 		u16 mode;
@@ -320,8 +350,12 @@ struct mlx5e_params {
 	bool rx_moder_use_cqe_mode;
 	bool tx_moder_use_cqe_mode;
 	u32 pflags;
+#ifdef HAVE_XDP_SUPPORT
 	struct bpf_prog *xdp_prog;
+#endif
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	struct mlx5e_xsk *xsk;
+#endif
 	unsigned int sw_mtu;
 	int hard_mtu;
 	bool ptp_rx;
@@ -352,6 +386,9 @@ enum {
 	MLX5E_RQ_STATE_MINI_CQE_ENHANCED,  /* set when enhanced mini_cqe_cap is used */
 	MLX5E_RQ_STATE_XSK, /* set to indicate an xsk rq */
 	MLX5E_NUM_RQ_STATES, /* Must be kept last */
+#if !defined(HAVE_PAGE_POOL_DEFRAG_PAGE) && !defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
+	MLX5E_RQ_STATE_CACHE_REDUCE_PENDING
+#endif
 };
 
 struct mlx5e_cq {
@@ -444,6 +481,9 @@ struct mlx5e_txqsq {
 	struct mlx5e_tx_mpwqe      mpwqe;
 
 	struct mlx5e_cq            cq;
+#ifdef HAVE_BASECODE_EXTRAS
+	struct mlx5e_cq_decomp     cqd;
+#endif
 
 	/* read only */
 	struct mlx5_wq_cyc         wq;
@@ -480,6 +520,26 @@ struct mlx5e_txqsq {
 	cqe_ts_to_ns               ptp_cyc2time;
 } ____cacheline_aligned_in_smp;
 
+#if !defined(HAVE_PAGE_POOL_DEFRAG_PAGE) && !defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
+struct  mlx5e_alloc_unit {
+#if !defined(HAVE_PAGE_POOL_GET_DMA_ADDR) || !defined(HAVE_XSK_BUFF_GET_FRAME_DMA)
+	dma_addr_t addr;
+#endif
+	union {
+		struct page *page;
+#ifdef HAVE_XSK_BUFF_ALLOC
+		struct xdp_buff *xsk;
+#else
+		struct {
+			u64 handle;
+			void *data;
+		} xsk;
+#endif
+	};
+};
+#endif
+
+#ifdef HAVE_XDP_SUPPORT
 struct mlx5e_xdp_info_fifo {
 	union mlx5e_xdp_info *xi;
 	u32 *cc;
@@ -512,7 +572,13 @@ struct mlx5e_xdpsq {
 	struct mlx5e_cq            cq;
 
 	/* read only */
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
 	struct xsk_buff_pool      *xsk_pool;
+#else
+	struct xdp_umem           *umem;
+#endif
+#endif
 	struct mlx5_wq_cyc         wq;
 	struct mlx5e_xdpsq_stats  *stats;
 	mlx5e_fp_xmit_xdp_frame_check xmit_xdp_frame_check;
@@ -535,6 +601,8 @@ struct mlx5e_xdpsq {
 	struct mlx5_wq_ctrl        wq_ctrl;
 	struct mlx5e_channel      *channel;
 } ____cacheline_aligned_in_smp;
+#endif /* HAVE_XDP_SUPPORT */
+
 
 struct mlx5e_ktls_resync_resp;
 
@@ -566,6 +634,7 @@ struct mlx5e_icosq {
 	struct work_struct         recover_work;
 } ____cacheline_aligned_in_smp;
 
+#if defined(HAVE_PAGE_POOL_DEFRAG_PAGE) || defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
 struct mlx5e_frag_page {
 	struct page *page;
 	u16 frags;
@@ -575,31 +644,79 @@ enum mlx5e_wqe_frag_flag {
 	MLX5E_WQE_FRAG_LAST_IN_PAGE,
 	MLX5E_WQE_FRAG_SKIP_RELEASE,
 };
+#endif
 
 struct mlx5e_wqe_frag_info {
-	union {
-		struct mlx5e_frag_page *frag_page;
-		struct xdp_buff **xskp;
-	};
+#if defined(HAVE_PAGE_POOL_DEFRAG_PAGE) || defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
+       union {
+              struct mlx5e_frag_page *frag_page;
+              struct xdp_buff **xskp;
+       };
+#else
+        struct mlx5e_alloc_unit *au;
+#endif
 	u32 offset;
+#if defined(HAVE_PAGE_POOL_DEFRAG_PAGE) || defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
 	u8 flags;
+#else
+	bool last_in_page;
+#endif
 };
 
+#if defined(HAVE_PAGE_POOL_DEFRAG_PAGE) || defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
 union mlx5e_alloc_units {
-	DECLARE_FLEX_ARRAY(struct mlx5e_frag_page, frag_pages);
-	DECLARE_FLEX_ARRAY(struct page *, pages);
-	DECLARE_FLEX_ARRAY(struct xdp_buff *, xsk_buffs);
+       DECLARE_FLEX_ARRAY(struct mlx5e_frag_page, frag_pages);
+       DECLARE_FLEX_ARRAY(struct page *, pages);
+       DECLARE_FLEX_ARRAY(struct xdp_buff *, xsk_buffs);
 };
+#endif
 
 struct mlx5e_mpw_info {
 	u16 consumed_strides;
 	DECLARE_BITMAP(skip_release_bitmap, MLX5_MPWRQ_MAX_PAGES_PER_WQE);
+#if defined(HAVE_PAGE_POOL_DEFRAG_PAGE) || defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
 	struct mlx5e_frag_page linear_page;
 	union mlx5e_alloc_units alloc_units;
+#else
+	struct mlx5e_alloc_unit alloc_units[];
+#endif
 };
 
 #define MLX5E_MAX_RX_FRAGS 4
 
+#if !defined(HAVE_PAGE_POOL_DEFRAG_PAGE) && !defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
+#define MLX5E_PAGE_CACHE_LOG_MAX_RQ_MULT	4
+#define MLX5E_PAGE_CACHE_REDUCE_WORK_INTERVAL	200 /* msecs */
+#define MLX5E_PAGE_CACHE_REDUCE_GRACE_PERIOD	1000 /* msecs */
+#define MLX5E_PAGE_CACHE_REDUCE_SUCCESSIVE_CNT	5
+
+struct mlx5e_page_cache_reduce {
+	struct delayed_work reduce_work;
+	u32 successive;
+	unsigned long next_ts;
+	unsigned long graceful_period;
+	unsigned long delay;
+
+	struct mlx5e_alloc_unit *pending;
+	u32 npages;
+};
+
+struct mlx5e_page_cache {
+	struct mlx5e_alloc_unit *page_cache;
+	int head;
+	u32 sz;
+	u32 lrs; /* least recently sampled */
+	u8 log_min_sz;
+	u8 log_max_sz;
+	struct mlx5e_page_cache_reduce reduce;
+};
+#endif
+
+static inline void mlx5e_put_page(struct page *page)
+{
+	put_page(page);
+}
+
 struct mlx5e_rq;
 typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq*, struct mlx5_cqe64*);
 typedef struct sk_buff *
@@ -630,22 +747,34 @@ struct mlx5e_rq_frags_info {
 	struct mlx5e_rq_frag_info arr[MLX5E_MAX_RX_FRAGS];
 	u8 num_frags;
 	u8 log_num_frags;
+#if defined(HAVE_PAGE_POOL_DEFRAG_PAGE) || defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
 	u16 wqe_bulk;
 	u16 refill_unit;
+#else
+	u8 wqe_bulk;
+#endif
 	u8 wqe_index_mask;
 };
 
 struct mlx5e_dma_info {
 	dma_addr_t addr;
+#if defined(HAVE_PAGE_POOL_DEFRAG_PAGE) || defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
 	union {
 		struct mlx5e_frag_page *frag_page;
 		struct page *page;
 	};
+#else
+	struct page *page;
+#endif
 };
 
 struct mlx5e_shampo_hd {
 	u32 mkey;
+#if defined(HAVE_PAGE_POOL_DEFRAG_PAGE) || defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
 	struct mlx5e_frag_page *pages;
+#else
+	struct mlx5e_alloc_unit    *au;
+#endif
 	u32 hd_per_wq;
 	u16 hd_per_wqe;
 	u16 pages_per_wq;
@@ -674,7 +803,11 @@ struct mlx5e_rq {
 		struct {
 			struct mlx5_wq_cyc          wq;
 			struct mlx5e_wqe_frag_info *frags;
+#if defined(HAVE_PAGE_POOL_DEFRAG_PAGE) || defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
 			union mlx5e_alloc_units    *alloc_units;
+#else
+			struct mlx5e_alloc_unit     *alloc_units;
+#endif
 			struct mlx5e_rq_frags_info  info;
 			mlx5e_fp_skb_from_cqe       skb_from_cqe;
 		} wqe;
@@ -700,6 +833,9 @@ struct mlx5e_rq {
 		} mpwqe;
 	};
 	struct {
+#if !defined(HAVE_XSK_BUFF_ALLOC) && defined(HAVE_XSK_ZERO_COPY_SUPPORT)
+		u16            umem_headroom;
+#endif
 		u16            headroom;
 		u32            frame0_sz;
 		u8             map_dir;   /* dma map direction */
@@ -710,6 +846,9 @@ struct mlx5e_rq {
 	struct mlx5e_rq_stats *stats;
 	struct mlx5e_cq        cq;
 	struct mlx5e_cq_decomp cqd;
+#if !defined(HAVE_PAGE_POOL_DEFRAG_PAGE) && !defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
+	struct mlx5e_page_cache page_cache;
+#endif
 	struct hwtstamp_config *tstamp;
 	struct mlx5_clock      *clock;
 	struct mlx5e_icosq    *icosq;
@@ -727,15 +866,24 @@ struct mlx5e_rq {
 
 	struct dim            *dim; /* Dynamic Interrupt Moderation */
 
+#ifdef HAVE_XDP_SUPPORT
 	/* XDP */
 	struct bpf_prog __rcu *xdp_prog;
 	struct mlx5e_xdpsq    *xdpsq;
+#endif
 	DECLARE_BITMAP(flags, 8);
 	struct page_pool      *page_pool;
-
 	/* AF_XDP zero-copy */
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+#ifndef HAVE_XSK_BUFF_ALLOC
+	struct zero_copy_allocator zca;
+#endif
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
 	struct xsk_buff_pool  *xsk_pool;
-
+#else
+	struct xdp_umem       *umem;
+#endif
+#endif /* HAVE_XSK_ZERO_COPY_SUPPORT */
 	struct work_struct     recover_work;
 
 	/* control */
@@ -760,11 +908,15 @@ enum mlx5e_channel_state {
 struct mlx5e_channel {
 	/* data path */
 	struct mlx5e_rq            rq;
+#ifdef HAVE_XDP_SUPPORT
 	struct mlx5e_xdpsq         rq_xdpsq;
+#endif
 	struct mlx5e_txqsq         sq[MLX5_MAX_NUM_TC];
 	struct mlx5e_icosq         icosq;   /* internal control operations */
 	struct mlx5e_txqsq __rcu * __rcu *qos_sqs;
+#ifdef HAVE_XDP_SUPPORT
 	bool                       xdp;
+#endif
 	struct napi_struct         napi;
 	struct device             *pdev;
 	struct net_device         *netdev;
@@ -773,12 +925,15 @@ struct mlx5e_channel {
 	u8                         num_tc;
 	u8                         lag_port;
 
+#ifdef HAVE_XDP_SUPPORT
 	/* XDP_REDIRECT */
 	struct mlx5e_xdpsq        *xdpsq;
-
+#endif
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	/* AF_XDP zero-copy */
 	struct mlx5e_rq            xskrq;
 	struct mlx5e_xdpsq         xsksq;
+#endif
 
 	/* Async ICOSQ */
 	struct mlx5e_icosq         async_icosq;
@@ -820,10 +975,16 @@ struct mlx5e_channel_stats {
 	struct mlx5e_ch_stats ch;
 	struct mlx5e_sq_stats sq[MLX5_MAX_NUM_TC];
 	struct mlx5e_rq_stats rq;
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	struct mlx5e_rq_stats xskrq;
+#endif
+#ifdef HAVE_XDP_SUPPORT
 	struct mlx5e_xdpsq_stats rq_xdpsq;
 	struct mlx5e_xdpsq_stats xdpsq;
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	struct mlx5e_xdpsq_stats xsksq;
+#endif
+#endif
 } ____cacheline_aligned_in_smp;
 
 struct mlx5e_ptp_stats {
@@ -859,6 +1020,7 @@ struct mlx5e_hv_vhca_stats_agent {
 };
 #endif
 
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 struct mlx5e_xsk {
 	/* XSK buffer pools are stored separately from channels,
 	 * because we don't want to lose them when channels are
@@ -866,10 +1028,15 @@ struct mlx5e_xsk {
 	 * distinguish between zero-copy and non-zero-copy UMEMs, so
 	 * rely on our mechanism.
 	 */
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
 	struct xsk_buff_pool **pools;
+#else
+	struct xdp_umem **umems;
+#endif
 	u16 refcnt;
 	bool ever_used;
 };
+#endif
 
 /* Temporary storage for variables that are allocated when struct mlx5e_priv is
  * initialized, and used where we can't allocate them because that functions
@@ -929,7 +1096,7 @@ struct mlx5e_priv {
 	struct mlx5e_channel_stats trap_stats;
 	struct mlx5e_ptp_stats     ptp_stats;
 	struct mlx5e_sq_stats      **htb_qos_sq_stats;
-	u16                        htb_max_qos_sqs;
+	u32                        htb_max_qos_sqs;
 	u16                        stats_nch;
 	u16                        max_nch;
 	u8                         max_opened_tc;
@@ -941,7 +1108,9 @@ struct mlx5e_priv {
 	struct notifier_block      events_nb;
 	struct notifier_block      blocking_events_nb;
 
+#ifdef HAVE_UDP_TUNNEL_NIC_INFO
 	struct udp_tunnel_nic_info nic_info;
+#endif
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	struct mlx5e_dcbx          dcbx;
 #endif
@@ -963,7 +1132,9 @@ struct mlx5e_priv {
 	struct mlx5e_delay_drop delay_drop;
 	struct devlink_health_reporter *tx_reporter;
 	struct devlink_health_reporter *rx_reporter;
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	struct mlx5e_xsk           xsk;
+#endif
 #if IS_ENABLED(CONFIG_PCI_HYPERV_INTERFACE)
 	struct mlx5e_hv_vhca_stats_agent stats_agent;
 #endif
@@ -978,7 +1149,9 @@ struct mlx5e_priv {
 
 struct mlx5e_dev {
 	struct mlx5e_priv *priv;
+#ifdef HAVE_NET_DEVICE_HAS_DEVLINK_PORT
 	struct devlink_port dl_port;
+#endif
 };
 
 struct mlx5e_rx_handlers {
@@ -1042,9 +1215,11 @@ int mlx5e_sysfs_create(struct net_device
 void mlx5e_sysfs_remove(struct net_device *dev);
 
 int mlx5e_setup_tc_mqprio(struct mlx5e_priv *priv,
-			  struct tc_mqprio_qopt_offload *mqprio);
+			  struct tc_mqprio_qopt_offload *mqprio
+);
 
 void mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats);
+
 void mlx5e_fold_sw_stats64(struct mlx5e_priv *priv, struct rtnl_link_stats64 *s);
 
 int mlx5e_self_test_num(struct mlx5e_priv *priv);
@@ -1056,7 +1231,21 @@ void mlx5e_set_rx_mode_work(struct work_
 int mlx5e_hwstamp_set(struct mlx5e_priv *priv, struct ifreq *ifr);
 int mlx5e_hwstamp_get(struct mlx5e_priv *priv, struct ifreq *ifr);
 int mlx5e_modify_rx_cqe_compression_locked(struct mlx5e_priv *priv, bool val, bool rx_filter);
+#ifdef HAVE_BASECODE_EXTRAS
+int mlx5e_modify_tx_cqe_compression_locked(struct mlx5e_priv *priv, bool val);
+#endif
 
+#if defined(HAVE_NDO_GET_PORT_PARENT_ID)
+#ifdef HAVE_BASECODE_EXTRAS
+#ifdef HAVE_DEVLINK_PORT_ATTRS_PCI_PF_SET
+void
+#else
+int
+#endif
+mlx5e_get_port_parent_id(struct net_device *dev,
+		                         struct netdev_phys_item_id *ppid);
+#endif
+#endif
 int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
 			  u16 vid);
 int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
@@ -1081,10 +1270,20 @@ bool mlx5e_reset_rx_channels_moderation(
 					bool dim_enabled, bool keep_dim_state);
 
 struct mlx5e_sq_param;
-int mlx5e_open_xdpsq(struct mlx5e_channel *c, struct mlx5e_params *params,
-		     struct mlx5e_sq_param *param, struct xsk_buff_pool *xsk_pool,
-		     struct mlx5e_xdpsq *sq, bool is_redirect);
+#ifdef HAVE_XDP_SUPPORT
 void mlx5e_close_xdpsq(struct mlx5e_xdpsq *sq);
+int mlx5e_open_xdpsq(struct mlx5e_channel *c, struct mlx5e_params *params,
+		     struct mlx5e_sq_param *param,
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
+		     struct xsk_buff_pool *xsk_pool,
+#else
+		     struct xdp_umem *umem,
+#endif
+#endif
+		     struct mlx5e_xdpsq *sq,
+                     bool is_redirect);
+#endif
 
 struct mlx5e_create_cq_param {
 	struct net_device *netdev;
@@ -1183,6 +1382,10 @@ void mlx5e_destroy_mdev_resources(struct
 int mlx5e_refresh_tirs(struct mlx5e_priv *priv, bool enable_uc_lb,
 		       bool enable_mc_lb);
 int mlx5e_modify_tirs_packet_merge(struct mlx5e_priv *priv);
+int mlx5e_modify_tirs_packet_merge_ctx(struct mlx5e_priv *priv, void *context);
+#ifdef HAVE_BASECODE_EXTRAS
+int mlx5e_update_lro(struct net_device *netdev, bool enable);
+#endif
 void mlx5e_mkey_set_relaxed_ordering(struct mlx5_core_dev *mdev, void *mkc);
 
 /* common netdev helpers */
@@ -1206,7 +1409,9 @@ int mlx5e_set_dev_port_mtu(struct mlx5e_
 int mlx5e_set_dev_port_mtu_ctx(struct mlx5e_priv *priv, void *context);
 int mlx5e_change_mtu(struct net_device *netdev, int new_mtu,
 		     mlx5e_fp_preactivate preactivate);
+#ifdef HAVE_UDP_TUNNEL_NIC_INFO_FULL
 void mlx5e_vxlan_set_netdev_info(struct mlx5e_priv *priv);
+#endif
 
 /* ethtool helpers */
 void mlx5e_ethtool_get_drvinfo(struct mlx5e_priv *priv,
@@ -1217,8 +1422,12 @@ int mlx5e_ethtool_get_sset_count(struct
 void mlx5e_ethtool_get_ethtool_stats(struct mlx5e_priv *priv,
 				     struct ethtool_stats *stats, u64 *data);
 void mlx5e_ethtool_get_ringparam(struct mlx5e_priv *priv,
+#if defined(HAVE_STRUCT_KERNEL_ETHTOOL_RINGPARAM) && defined(HAVE_GET_RINGPARAM_GET_4_PARAMS)
 				 struct ethtool_ringparam *param,
 				 struct kernel_ethtool_ringparam *kernel_param);
+#else
+				 struct ethtool_ringparam *param);
+#endif
 int mlx5e_ethtool_set_ringparam(struct mlx5e_priv *priv,
 				struct ethtool_ringparam *param,
 				struct netlink_ext_ack *extack);
@@ -1227,13 +1436,21 @@ void mlx5e_ethtool_get_channels(struct m
 int mlx5e_ethtool_set_channels(struct mlx5e_priv *priv,
 			       struct ethtool_channels *ch);
 int mlx5e_ethtool_get_coalesce(struct mlx5e_priv *priv,
+#ifdef HAVE_NDO_GET_COALESCE_GET_4_PARAMS
 			       struct ethtool_coalesce *coal,
 			       struct kernel_ethtool_coalesce *kernel_coal,
 			       struct netlink_ext_ack *extack);
+#else
+			       struct ethtool_coalesce *coal);
+#endif
 int mlx5e_ethtool_set_coalesce(struct mlx5e_priv *priv,
+#ifdef HAVE_NDO_GET_COALESCE_GET_4_PARAMS
 			       struct ethtool_coalesce *coal,
 			       struct kernel_ethtool_coalesce *kernel_coal,
 			       struct netlink_ext_ack *extack);
+#else
+			       struct ethtool_coalesce *coal);
+#endif
 int mlx5e_get_per_queue_coalesce(struct net_device *dev, u32 queue,
 				 struct ethtool_coalesce *coal);
 int mlx5e_set_per_queue_coalesce(struct net_device *dev, u32 queue,
@@ -1241,9 +1458,19 @@ int mlx5e_set_per_queue_coalesce(struct
 u32 mlx5e_ethtool_get_rxfh_key_size(struct mlx5e_priv *priv);
 u32 mlx5e_ethtool_get_rxfh_indir_size(struct mlx5e_priv *priv);
 int mlx5e_ethtool_get_ts_info(struct mlx5e_priv *priv,
+#ifdef HAVE_STRUCT_KERNEL_ETHTOOL_TS_INFO
 			      struct kernel_ethtool_ts_info *info);
+#else
+			      struct ethtool_ts_info *info);
+#endif
 int mlx5e_ethtool_flash_device(struct mlx5e_priv *priv,
 			       struct ethtool_flash *flash);
+#ifdef HAVE_TC_SETUP_CB_EGDEV_REGISTER
+#ifndef HAVE_TC_BLOCK_OFFLOAD
+int mlx5e_setup_tc(struct net_device *dev, enum tc_setup_type type,
+		   void *type_data);
+#endif
+#endif
 
 /* mlx5e generic netdev management API */
 static inline bool
@@ -1268,7 +1495,11 @@ int mlx5e_netdev_change_profile(struct m
 				const struct mlx5e_profile *new_profile, void *new_ppriv);
 void mlx5e_netdev_attach_nic_profile(struct mlx5e_priv *priv);
 void mlx5e_set_netdev_mtu_boundaries(struct mlx5e_priv *priv);
-void mlx5e_build_nic_params(struct mlx5e_priv *priv, struct mlx5e_xsk *xsk, u16 mtu);
+void mlx5e_build_nic_params(struct mlx5e_priv *priv,
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+			   struct mlx5e_xsk *xsk,
+#endif
+			   u16 mtu);
 
 int mlx5e_get_dump_flag(struct net_device *netdev, struct ethtool_dump *dump);
 int mlx5e_get_dump_data(struct net_device *netdev, struct ethtool_dump *dump,
@@ -1281,7 +1512,14 @@ static inline bool mlx5e_dropless_rq_sup
 		MLX5_CAP_GEN(mdev, general_notification_event));
 }
 
+#ifdef HAVE_NDO_UDP_TUNNEL_ADD
+void mlx5e_add_vxlan_port(struct net_device *netdev, struct udp_tunnel_info *ti);
+void mlx5e_del_vxlan_port(struct net_device *netdev, struct udp_tunnel_info *ti);
+#endif
+
+#ifdef HAVE_XDP_SET_FEATURES_FLAG
 void mlx5e_set_xdp_feature(struct net_device *netdev);
+#endif
 netdev_features_t mlx5e_features_check(struct sk_buff *skb,
 				       struct net_device *netdev,
 				       netdev_features_t features);
