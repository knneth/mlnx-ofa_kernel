From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: net/sunrpc/xprtrdma/verbs.c

---
 net/sunrpc/xprtrdma/verbs.c | 78 +++++++++++++++++++++++++++++++++++++
 1 file changed, 78 insertions(+)

--- a/net/sunrpc/xprtrdma/verbs.c
+++ b/net/sunrpc/xprtrdma/verbs.c
@@ -49,6 +49,13 @@
  *  o buffer memory
  */
 
+#include <linux/version.h>
+
+#if ((LINUX_VERSION_CODE >= KERNEL_VERSION(4,15,0)) || \
+	(defined(RHEL_MAJOR) && ((RHEL_MAJOR == 7 && RHEL_MINOR >= 6) || \
+	RHEL_MAJOR >= 8)))
+#include <asm-generic/barrier.h>
+#endif
 #include <linux/bitops.h>
 #include <linux/interrupt.h>
 #include <linux/slab.h>
@@ -61,8 +68,15 @@
 #include <rdma/ib_cm.h>
 
 #include "xprt_rdma.h"
+#ifdef HAVE_TRACE_RPCRDMA_H
 #include <trace/events/rpcrdma.h>
+#endif
 
+#if IS_ENABLED(CONFIG_SUNRPC_DEBUG)
+#ifndef RPCDBG_FACILITY
+#define RPCDBG_FACILITY    RPCDBG_TRANS
+#endif
+#endif
 static int rpcrdma_sendctxs_create(struct rpcrdma_xprt *r_xprt);
 static void rpcrdma_sendctxs_destroy(struct rpcrdma_xprt *r_xprt);
 static void rpcrdma_sendctx_put_locked(struct rpcrdma_xprt *r_xprt,
@@ -147,7 +161,9 @@ static void rpcrdma_wc_send(struct ib_cq
 	struct rpcrdma_xprt *r_xprt = cq->cq_context;
 
 	/* WARNING: Only wr_cqe and status are reliable at this point */
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_wc_send(wc, &sc->sc_cid);
+#endif
 	rpcrdma_sendctx_put_locked(r_xprt, sc);
 	rpcrdma_flush_disconnect(r_xprt, wc);
 }
@@ -166,7 +182,9 @@ static void rpcrdma_wc_receive(struct ib
 	struct rpcrdma_xprt *r_xprt = cq->cq_context;
 
 	/* WARNING: Only wr_cqe and status are reliable at this point */
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_wc_receive(wc, &rep->rr_cid);
+#endif
 	--r_xprt->rx_ep->re_receive_count;
 	if (wc->status != IB_WC_SUCCESS)
 		goto out_flushed;
@@ -224,6 +242,9 @@ static void rpcrdma_update_cm_private(st
 static int
 rpcrdma_cm_event_handler(struct rdma_cm_id *id, struct rdma_cm_event *event)
 {
+#ifndef HAVE_RPCRDMA_RN_REGISTER
+	struct sockaddr *sap = (struct sockaddr *)&id->route.addr.dst_addr;
+#endif
 	struct rpcrdma_ep *ep = id->context;
 
 	might_sleep();
@@ -242,6 +263,16 @@ rpcrdma_cm_event_handler(struct rdma_cm_
 		ep->re_async_rc = -ENETUNREACH;
 		complete(&ep->re_done);
 		return 0;
+#ifndef HAVE_RPCRDMA_RN_REGISTER
+	case RDMA_CM_EVENT_DEVICE_REMOVAL:
+		pr_info("rpcrdma: removing device %s for %pISpc\n",
+				ep->re_id->device->name, sap);
+		switch (xchg(&ep->re_connect_status, -ENODEV)) {
+			case 0: goto wake_connect_worker;
+			case 1: goto disconnected;
+		}
+		return 0;
+#endif
 	case RDMA_CM_EVENT_ADDR_CHANGE:
 		ep->re_connect_status = -ENODEV;
 		goto disconnected;
@@ -249,7 +280,9 @@ rpcrdma_cm_event_handler(struct rdma_cm_
 		rpcrdma_ep_get(ep);
 		ep->re_connect_status = 1;
 		rpcrdma_update_cm_private(ep, &event->param.conn);
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_inline_thresh(ep);
+#endif
 		wake_up_all(&ep->re_connect_wait);
 		break;
 	case RDMA_CM_EVENT_CONNECT_ERROR:
@@ -277,13 +310,17 @@ disconnected:
 	return 0;
 }
 
+#ifdef HAVE_RPCRDMA_RN_REGISTER
 static void rpcrdma_ep_removal_done(struct rpcrdma_notification *rn)
 {
 	struct rpcrdma_ep *ep = container_of(rn, struct rpcrdma_ep, re_rn);
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_device_removal(ep->re_id);
+#endif
 	xprt_force_disconnect(ep->re_xprt);
 }
+#endif
 
 static struct rdma_cm_id *rpcrdma_create_id(struct rpcrdma_xprt *r_xprt,
 					    struct rpcrdma_ep *ep)
@@ -324,9 +361,11 @@ static struct rdma_cm_id *rpcrdma_create
 	if (rc)
 		goto out;
 
+#ifdef HAVE_RPCRDMA_RN_REGISTER
 	rc = rpcrdma_rn_register(id->device, &ep->re_rn, rpcrdma_ep_removal_done);
 	if (rc)
 		goto out;
+#endif
 
 	return id;
 
@@ -355,7 +394,9 @@ static void rpcrdma_ep_destroy(struct kr
 		ib_dealloc_pd(ep->re_pd);
 	ep->re_pd = NULL;
 
+#ifdef HAVE_RPCRDMA_RN_REGISTER
 	rpcrdma_rn_unregister(ep->re_id->device, &ep->re_rn);
+#endif
 
 	kfree(ep);
 	module_put(THIS_MODULE);
@@ -542,7 +583,9 @@ int rpcrdma_xprt_connect(struct rpcrdma_
 	frwr_wp_create(r_xprt);
 
 out:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_connect(r_xprt, rc);
+#endif
 	return rc;
 }
 
@@ -567,7 +610,9 @@ void rpcrdma_xprt_disconnect(struct rpcr
 
 	id = ep->re_id;
 	rc = rdma_disconnect(id);
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_disconnect(r_xprt, rc);
+#endif
 
 	rpcrdma_xprt_drain(r_xprt);
 	rpcrdma_reps_unmap(r_xprt);
@@ -652,6 +697,9 @@ static int rpcrdma_sendctxs_create(struc
 		buf->rb_sc_ctxs[i] = sc;
 	}
 
+#ifndef HAVE_XPRT_WAIT_FOR_BUFFER_SPACE_RQST_ARG
+	buf->rb_flags = 0;
+#endif
 	buf->rb_sc_head = 0;
 	buf->rb_sc_tail = 0;
 	return 0;
@@ -706,7 +754,11 @@ out_emptyq:
 	 * completions recently. This is a sign the Send Queue is
 	 * backing up. Cause the caller to pause and try again.
 	 */
+#ifdef HAVE_XPRT_WAIT_FOR_BUFFER_SPACE_RQST_ARG
 	xprt_wait_for_buffer_space(&r_xprt->rx_xprt);
+#else
+	set_bit(RPCRDMA_BUF_F_EMPTY_SCQ, &buf->rb_flags);
+#endif
 	r_xprt->rx_stats.empty_sendctx_q++;
 	return NULL;
 }
@@ -742,7 +794,14 @@ static void rpcrdma_sendctx_put_locked(s
 	/* Paired with READ_ONCE */
 	smp_store_release(&buf->rb_sc_tail, next_tail);
 
+#ifdef HAVE_XPRT_WAIT_FOR_BUFFER_SPACE_RQST_ARG
 	xprt_write_space(&r_xprt->rx_xprt);
+#else
+	if (test_and_clear_bit(RPCRDMA_BUF_F_EMPTY_SCQ, &buf->rb_flags)) {
+		smp_mb__after_atomic();
+		xprt_write_space(&r_xprt->rx_xprt);
+	}
+#endif
 }
 
 static void
@@ -776,7 +835,12 @@ rpcrdma_mrs_create(struct rpcrdma_xprt *
 	}
 
 	r_xprt->rx_stats.mrs_allocated += count;
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_createmrs(r_xprt, count);
+#endif
+#ifndef HAVE_XPRT_WAIT_FOR_BUFFER_SPACE_RQST_ARG
+	xprt_write_space(&r_xprt->rx_xprt);
+#endif
 }
 
 static void
@@ -788,7 +852,9 @@ rpcrdma_mr_refresh_worker(struct work_st
 						   rx_buf);
 
 	rpcrdma_mrs_create(r_xprt);
+#ifdef HAVE_XPRT_WAIT_FOR_BUFFER_SPACE_RQST_ARG
 	xprt_write_space(&r_xprt->rx_xprt);
+#endif
 }
 
 /**
@@ -806,7 +872,11 @@ void rpcrdma_mrs_refresh(struct rpcrdma_
 	 */
 	if (ep->re_connect_status != 1)
 		return;
+#ifdef HAVE_XPRT_RECONNECT_DELAY
 	queue_work(system_highpri_wq, &buf->rb_refresh_worker);
+#else
+	schedule_work(&buf->rb_refresh_worker);
+#endif
 }
 
 /**
@@ -1309,7 +1379,9 @@ bool __rpcrdma_regbuf_dma_map(struct rpc
 	rb->rg_iov.addr = ib_dma_map_single(device, rdmab_data(rb),
 					    rdmab_length(rb), rb->rg_direction);
 	if (ib_dma_mapping_error(device, rdmab_addr(rb))) {
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_dma_maperr(rdmab_addr(rb));
+#endif
 		return false;
 	}
 
@@ -1378,7 +1450,9 @@ void rpcrdma_post_recvs(struct rpcrdma_x
 		}
 
 		rep->rr_cid.ci_queue_id = ep->re_attr.recv_cq->res.id;
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_post_recv(&rep->rr_cid);
+#endif
 		rep->rr_recv_wr.next = wr;
 		wr = &rep->rr_recv_wr;
 		--needed;
@@ -1390,7 +1464,9 @@ void rpcrdma_post_recvs(struct rpcrdma_x
 	rc = ib_post_recv(ep->re_id->qp, wr,
 			  (const struct ib_recv_wr **)&bad_wr);
 	if (rc) {
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_post_recvs_err(r_xprt, rc);
+#endif
 		for (wr = bad_wr; wr;) {
 			struct rpcrdma_rep *rep;
 
@@ -1404,7 +1480,9 @@ void rpcrdma_post_recvs(struct rpcrdma_x
 		complete(&ep->re_done);
 
 out:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_post_recvs(r_xprt, count);
+#endif
 	ep->re_receive_count += count;
 	return;
 }
