From: Alaa Hleihel <alaa@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/infiniband/hw/mlx5/main.c

Change-Id: I3b04877a36873c5bb5ce48c7782fac52d1d0c5b2
---
 drivers/infiniband/hw/mlx5/main.c | 49 +++++++++++++++++++++++++++++++++++++++
 1 file changed, 49 insertions(+)

--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@ -65,7 +65,9 @@
 #include "user_exp.h"
 #include <linux/mlx5/vport.h>
 #include <linux/mlx5/capi.h>
+#ifdef HAVE_MM_CONTEXT_ADD_COPRO
 #include <linux/mmu_context.h>
+#endif
 
 #define DRIVER_NAME "mlx5_ib"
 #define DRIVER_VERSION	"4.3-3.0.2"
@@ -145,13 +147,17 @@ static int mlx5_netdev_event(struct noti
 	case NETDEV_CHANGE:
 	case NETDEV_UP:
 	case NETDEV_DOWN: {
+#ifdef HAVE_NETDEV_MASTER_UPPER_DEV_GET
 		struct net_device *lag_ndev = mlx5_lag_get_roce_netdev(ibdev->mdev);
+#endif
 		struct net_device *upper = NULL;
 
+#ifdef HAVE_NETDEV_MASTER_UPPER_DEV_GET
 		if (lag_ndev) {
 			upper = netdev_master_upper_dev_get(lag_ndev);
 			dev_put(lag_ndev);
 		}
+#endif
 
 		if ((upper == ndev || (!upper && ndev == ibdev->roce.netdev))
 		    && ibdev->ib_active) {
@@ -764,8 +770,10 @@ int mlx5_ib_query_device(struct ib_devic
 	if (MLX5_CAP_GEN(mdev, cd))
 		props->device_cap_flags |= IB_DEVICE_CROSS_CHANNEL;
 
+#ifdef HAVE_NDO_SET_VF_MAC
 	if (!mlx5_core_is_pf(mdev))
 		props->device_cap_flags |= IB_DEVICE_VIRTUAL_FUNCTION;
+#endif
 
 	if (mlx5_ib_port_link_layer(ibdev, 1) ==
 	    IB_LINK_LAYER_ETHERNET) {
@@ -1043,7 +1051,9 @@ static int mlx5_query_hca_port(struct ib
 	props->qkey_viol_cntr	= rep->qkey_violation_counter;
 	props->subnet_timeout	= rep->subnet_timeout;
 	props->init_type_reply	= rep->init_type_reply;
+#ifdef HAVE_NDO_SET_VF_MAC
 	props->grh_required	= rep->grh_required;
+#endif
 
 	err = mlx5_query_port_link_width_oper(mdev, &ib_link_width_oper, port);
 	if (err)
@@ -1447,7 +1457,9 @@ static int alloc_capi_context(struct mlx
 		goto out_mm;
 	}
 
+#ifdef HAVE_MM_CONTEXT_ADD_COPRO
 	mm_context_add_copro(cctx->mm);
+#endif
 	return 0;
 
 out_mm:
@@ -1465,7 +1477,9 @@ static int free_capi_context(struct mlx5
 	err = mlx5_core_destroy_pec(dev->mdev, cctx->pasid);
 	if (err)
 		mlx5_ib_warn(dev, "destroy pec failed\n");
+#ifdef HAVE_MM_CONTEXT_ADD_COPRO
 	mm_context_remove_copro(cctx->mm);
+#endif
 	mmdrop(cctx->mm);
 	return err;
 }
@@ -1726,6 +1740,7 @@ static int get_index(unsigned long offse
 	return get_arg(offset);
 }
 
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined(HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 static void  mlx5_ib_vma_open(struct vm_area_struct *area)
 {
 	/* vma_open is called when a new VMA is created on top of our VMA.  This
@@ -1788,6 +1803,17 @@ void mlx5_ib_set_vma_data(struct vm_area
 	mutex_unlock(&ctx->vma_private_list_mutex);
 }
 
+#else
+void mlx5_ib_set_vma_data(struct vm_area_struct *vma,
+			  struct mlx5_ib_ucontext *ctx,
+			  struct mlx5_ib_vma_private_data *vma_prv)
+{
+	/* In case vma->vm_ops is not supported just free the vma_prv */
+	kfree(vma_prv);
+}
+#endif
+
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 static void mlx5_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)
 {
 	int ret;
@@ -1846,6 +1872,7 @@ static void mlx5_ib_disassociate_ucontex
 	mmput(owning_mm);
 	put_task_struct(owning_process);
 }
+#endif /* defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED) */
 
 static inline char *mmap_cmd2str(enum mlx5_ib_mmap_cmd cmd)
 {
@@ -1872,6 +1899,9 @@ static int uar_mmap(struct mlx5_ib_dev *
 	phys_addr_t pfn, pa;
 	pgprot_t prot;
 	int uars_per_page;
+#if defined(CONFIG_X86) && !defined(HAVE_PAT_ENABLED_AS_FUNCTION)
+	pgprot_t tmp_prot = __pgprot(0);
+#endif
 
 	if (vma->vm_end - vma->vm_start != PAGE_SIZE)
 		return -EINVAL;
@@ -1888,7 +1918,11 @@ static int uar_mmap(struct mlx5_ib_dev *
 	case MLX5_IB_MMAP_WC_PAGE:
 /* Some architectures don't support WC memory */
 #if defined(CONFIG_X86)
+#ifdef HAVE_PAT_ENABLED_AS_FUNCTION
 		if (!pat_enabled())
+#else
+		if (pgprot_val(pgprot_writecombine(tmp_prot)) == pgprot_val(pgprot_noncached(tmp_prot)))
+#endif
 			return -EPERM;
 #elif !(defined(CONFIG_PPC) || ((defined(CONFIG_ARM) || defined(CONFIG_ARM64)) && defined(CONFIG_MMU)))
 			return -EPERM;
@@ -1897,6 +1931,7 @@ static int uar_mmap(struct mlx5_ib_dev *
 	case MLX5_IB_MMAP_REGULAR_PAGE:
 		/* For MLX5_IB_MMAP_REGULAR_PAGE do the best effort to get WC */
 		prot = pgprot_writecombine(vma->vm_page_prot);
+#if defined(MIDR_CPU_MODEL_MASK)
 #if defined(CONFIG_ARM64)
 		/*
 		 * Fix up arm64 braindamage of using NORMAL_NC for write
@@ -1904,11 +1939,14 @@ static int uar_mmap(struct mlx5_ib_dev *
 		 * purpose. Needed on ThunderX2.
 		 */
 		switch (read_cpuid_id() & MIDR_CPU_MODEL_MASK) {
+#if defined(ARM_CPU_IMP_BRCM) && defined(BRCM_CPU_PART_VULCAN)
 		case MIDR_CPU_MODEL(ARM_CPU_IMP_BRCM, BRCM_CPU_PART_VULCAN):
+#endif
 		case MIDR_CPU_MODEL(0x43, 0x0af):  /* Cavium ThunderX2 */
 			prot = __pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_DEVICE_GRE) | PTE_PXN | PTE_UXN);
 		}
 #endif
+#endif
 		break;
 	case MLX5_IB_MMAP_NC_PAGE:
 		prot = pgprot_noncached(vma->vm_page_prot);
@@ -4675,6 +4713,9 @@ void *__mlx5_ib_add(struct mlx5_core_dev
 	dev->ib_dev.exp_query_device	= mlx5_ib_exp_query_device;
 	dev->ib_dev.exp_query_mkey      = mlx5_ib_exp_query_mkey;
 	dev->ib_dev.exp_create_qp	= mlx5_ib_exp_create_qp;
+#ifdef HAVE_MM_STRUCT_FREE_AREA_CACHE
+	dev->ib_dev.exp_get_unmapped_area = mlx5_ib_exp_get_unmapped_area;
+#endif
 	dev->ib_dev.resize_cq		= mlx5_ib_resize_cq;
 	dev->ib_dev.destroy_cq		= mlx5_ib_destroy_cq;
 	dev->ib_dev.poll_cq		= mlx5_ib_poll_cq;
@@ -4700,14 +4741,22 @@ void *__mlx5_ib_add(struct mlx5_core_dev
 	dev->ib_dev.query_counter_set   = mlx5_ib_query_counter_set;
 	dev->ib_dev.describe_counter_set = mlx5_ib_describe_counter_set;
 
+#ifdef HAVE_NDO_SET_VF_MAC
 	if (mlx5_core_is_pf(mdev)) {
 		dev->ib_dev.get_vf_config	= mlx5_ib_get_vf_config;
+#ifdef HAVE_LINKSTATE
 		dev->ib_dev.set_vf_link_state	= mlx5_ib_set_vf_link_state;
+#endif
 		dev->ib_dev.get_vf_stats	= mlx5_ib_get_vf_stats;
+#ifdef HAVE_IFLA_VF_IB_NODE_PORT_GUID
 		dev->ib_dev.set_vf_guid		= mlx5_ib_set_vf_guid;
+#endif
 	}
+#endif
 
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 	dev->ib_dev.disassociate_ucontext = mlx5_ib_disassociate_ucontext;
+#endif
 
 	if (MLX5_CAP_GEN(mdev, nvmf_target_offload)) {
 		dev->ib_dev.create_nvmf_backend_ctrl  = mlx5_ib_create_nvmf_backend_ctrl;
