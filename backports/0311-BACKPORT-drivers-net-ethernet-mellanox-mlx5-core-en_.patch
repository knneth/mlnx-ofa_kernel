From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en_stats.c

Change-Id: I796f838a006faa2e2efa5308103de686b56e8d04
---
 .../ethernet/mellanox/mlx5/core/en_stats.c    | 167 ++++++++++++++++--
 1 file changed, 156 insertions(+), 11 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en_stats.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_stats.c
@@ -36,8 +36,28 @@
 #include "en_accel/en_accel.h"
 #include "en/ptp.h"
 #include "en/port.h"
-
+#ifdef HAVE_NET_PAGE_POOL_OLD_H
+#include <net/page_pool.h>
+#endif
+#ifdef HAVE_NET_PAGE_POOL_TYPES_H
+#include <net/page_pool/types.h>
+#ifdef CONFIG_PAGE_POOL_STATS
 #include <net/page_pool/helpers.h>
+#endif
+#endif
+
+#ifndef HAVE_ETHTOOL_SPRINTF
+static void ethtool_sprintf(u8 **data, const char *fmt, ...)
+{
+	va_list args;
+
+	va_start(args, fmt);
+	vsnprintf(*data, ETH_GSTRING_LEN, fmt, args);
+	va_end(args);
+
+	*data += ETH_GSTRING_LEN;
+}
+#endif
 
 void mlx5e_ethtool_put_stat(u64 **data, u64 val)
 {
@@ -122,7 +142,7 @@ static const struct counter_desc sw_stat
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_mpwqe_blks) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_mpwqe_pkts) },
 
-#ifdef CONFIG_MLX5_EN_TLS
+#if defined(CONFIG_MLX5_EN_TLS)
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_tls_encrypted_packets) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_tls_encrypted_bytes) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_tls_ooo) },
@@ -152,6 +172,7 @@ static const struct counter_desc sw_stat
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_csum_complete_tail) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_csum_complete_tail_slow) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_csum_unnecessary_inner) },
+#ifdef HAVE_XDP_SUPPORT
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_xdp_drop) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_xdp_redirect) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_xdp_tx_xmit) },
@@ -161,6 +182,7 @@ static const struct counter_desc sw_stat
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_xdp_tx_full) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_xdp_tx_err) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_xdp_tx_cqe) },
+#endif
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_csum_none) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_csum_partial) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_csum_partial_inner) },
@@ -171,6 +193,7 @@ static const struct counter_desc sw_stat
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_cqes) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_queue_wake) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_cqe_err) },
+#ifdef HAVE_XDP_SUPPORT
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_xdp_xmit) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_xdp_mpwqe) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_xdp_inlnw) },
@@ -178,6 +201,11 @@ static const struct counter_desc sw_stat
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_xdp_full) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_xdp_err) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_xdp_cqes) },
+#endif
+#ifdef HAVE_BASECODE_EXTRAS
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_cqe_compress_blks) },
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_cqe_compress_pkts) },
+#endif
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_wqe_err) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_mpwqe_filler_cqes) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_mpwqe_filler_strides) },
@@ -185,6 +213,16 @@ static const struct counter_desc sw_stat
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_buff_alloc_err) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_cqe_compress_blks) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_cqe_compress_pkts) },
+#if !defined(HAVE_PAGE_POOL_DEFRAG_PAGE) && !defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_cache_reuse) },
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_cache_full) },
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_cache_empty) },
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_cache_busy) },
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_cache_ext) },
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_cache_rdc) },
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_cache_alloc) },
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_cache_waive) },
+#endif
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_congst_umr) },
 #ifdef CONFIG_MLX5_EN_ARFS
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_arfs_add) },
@@ -194,6 +232,7 @@ static const struct counter_desc sw_stat
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_arfs_err) },
 #endif
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_recover) },
+#ifdef CONFIG_PAGE_POOL_STATS
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_pp_alloc_fast) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_pp_alloc_slow) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_pp_alloc_slow_high_order) },
@@ -205,6 +244,7 @@ static const struct counter_desc sw_stat
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_pp_recycle_ring) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_pp_recycle_ring_full) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_pp_recycle_released_ref) },
+#endif
 #ifdef CONFIG_MLX5_EN_TLS
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_tls_decrypted_packets) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_tls_decrypted_bytes) },
@@ -274,6 +314,7 @@ static MLX5E_DECLARE_STATS_GRP_OP_FILL_S
 							    sw_stats_desc, i));
 }
 
+#ifdef HAVE_XDP_SUPPORT
 static void mlx5e_stats_grp_sw_update_stats_xdp_red(struct mlx5e_sw_stats *s,
 						    struct mlx5e_xdpsq_stats *xdpsq_red_stats)
 {
@@ -285,7 +326,9 @@ static void mlx5e_stats_grp_sw_update_st
 	s->tx_xdp_err   += xdpsq_red_stats->err;
 	s->tx_xdp_cqes  += xdpsq_red_stats->cqes;
 }
+#endif
 
+#ifdef HAVE_XDP_SUPPORT
 static void mlx5e_stats_grp_sw_update_stats_xdpsq(struct mlx5e_sw_stats *s,
 						  struct mlx5e_xdpsq_stats *xdpsq_stats)
 {
@@ -297,7 +340,8 @@ static void mlx5e_stats_grp_sw_update_st
 	s->rx_xdp_tx_err   += xdpsq_stats->err;
 	s->rx_xdp_tx_cqe   += xdpsq_stats->cqes;
 }
-
+#endif
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 static void mlx5e_stats_grp_sw_update_stats_xsksq(struct mlx5e_sw_stats *s,
 						  struct mlx5e_xdpsq_stats *xsksq_stats)
 {
@@ -331,6 +375,7 @@ static void mlx5e_stats_grp_sw_update_st
 	s->rx_xsk_cqe_compress_pkts      += xskrq_stats->cqe_compress_pkts;
 	s->rx_xsk_congst_umr             += xskrq_stats->congst_umr;
 }
+#endif
 
 static void mlx5e_stats_grp_sw_update_stats_rq_stats(struct mlx5e_sw_stats *s,
 						     struct mlx5e_rq_stats *rq_stats)
@@ -355,8 +400,10 @@ static void mlx5e_stats_grp_sw_update_st
 	s->rx_csum_complete_tail_slow += rq_stats->csum_complete_tail_slow;
 	s->rx_csum_unnecessary        += rq_stats->csum_unnecessary;
 	s->rx_csum_unnecessary_inner  += rq_stats->csum_unnecessary_inner;
+#ifdef HAVE_XDP_SUPPORT
 	s->rx_xdp_drop                += rq_stats->xdp_drop;
 	s->rx_xdp_redirect            += rq_stats->xdp_redirect;
+#endif
 	s->rx_wqe_err                 += rq_stats->wqe_err;
 	s->rx_mpwqe_filler_cqes       += rq_stats->mpwqe_filler_cqes;
 	s->rx_mpwqe_filler_strides    += rq_stats->mpwqe_filler_strides;
@@ -364,6 +411,16 @@ static void mlx5e_stats_grp_sw_update_st
 	s->rx_buff_alloc_err          += rq_stats->buff_alloc_err;
 	s->rx_cqe_compress_blks       += rq_stats->cqe_compress_blks;
 	s->rx_cqe_compress_pkts       += rq_stats->cqe_compress_pkts;
+#if !defined(HAVE_PAGE_POOL_DEFRAG_PAGE) && !defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
+	s->rx_cache_reuse             += rq_stats->cache_reuse;
+	s->rx_cache_full              += rq_stats->cache_full;
+	s->rx_cache_empty             += rq_stats->cache_empty;
+	s->rx_cache_busy              += rq_stats->cache_busy;
+	s->rx_cache_ext		      += rq_stats->cache_ext;
+	s->rx_cache_rdc   	      += rq_stats->cache_rdc;
+	s->rx_cache_alloc 	      += rq_stats->cache_alloc;
+	s->rx_cache_waive             += rq_stats->cache_waive;
+#endif
 	s->rx_congst_umr              += rq_stats->congst_umr;
 #ifdef CONFIG_MLX5_EN_ARFS
 	s->rx_arfs_add                += rq_stats->arfs_add;
@@ -373,6 +430,7 @@ static void mlx5e_stats_grp_sw_update_st
 	s->rx_arfs_err                += rq_stats->arfs_err;
 #endif
 	s->rx_recover                 += rq_stats->recover;
+#ifdef CONFIG_PAGE_POOL_STATS
 	s->rx_pp_alloc_fast          += rq_stats->pp_alloc_fast;
 	s->rx_pp_alloc_slow          += rq_stats->pp_alloc_slow;
 	s->rx_pp_alloc_empty         += rq_stats->pp_alloc_empty;
@@ -384,6 +442,7 @@ static void mlx5e_stats_grp_sw_update_st
 	s->rx_pp_recycle_ring			+= rq_stats->pp_recycle_ring;
 	s->rx_pp_recycle_ring_full		+= rq_stats->pp_recycle_ring_full;
 	s->rx_pp_recycle_released_ref		+= rq_stats->pp_recycle_released_ref;
+#endif
 #ifdef CONFIG_MLX5_EN_TLS
 	s->rx_tls_decrypted_packets   += rq_stats->tls_decrypted_packets;
 	s->rx_tls_decrypted_bytes     += rq_stats->tls_decrypted_bytes;
@@ -431,7 +490,7 @@ static void mlx5e_stats_grp_sw_update_st
 	s->tx_csum_partial_inner    += sq_stats->csum_partial_inner;
 	s->tx_csum_none             += sq_stats->csum_none;
 	s->tx_csum_partial          += sq_stats->csum_partial;
-#ifdef CONFIG_MLX5_EN_TLS
+#if defined(CONFIG_MLX5_EN_TLS)
 	s->tx_tls_encrypted_packets += sq_stats->tls_encrypted_packets;
 	s->tx_tls_encrypted_bytes   += sq_stats->tls_encrypted_bytes;
 	s->tx_tls_ooo               += sq_stats->tls_ooo;
@@ -443,6 +502,10 @@ static void mlx5e_stats_grp_sw_update_st
 	s->tx_tls_drop_bypass_req   += sq_stats->tls_drop_bypass_req;
 #endif
 	s->tx_cqes                  += sq_stats->cqes;
+#ifdef HAVE_BASECODE_EXTRAS
+	s->tx_cqe_compress_blks += sq_stats->cqe_compress_blks;
+	s->tx_cqe_compress_pkts += sq_stats->cqe_compress_pkts;
+#endif
 }
 
 static void mlx5e_stats_grp_sw_update_stats_ptp(struct mlx5e_priv *priv,
@@ -490,6 +553,7 @@ static void mlx5e_stats_grp_sw_update_st
 	}
 }
 
+#ifdef CONFIG_PAGE_POOL_STATS
 static void mlx5e_stats_update_stats_rq_page_pool(struct mlx5e_channel *c)
 {
 	struct mlx5e_rq_stats *rq_stats = c->rq.stats;
@@ -512,6 +576,11 @@ static void mlx5e_stats_update_stats_rq_
 	rq_stats->pp_recycle_ring_full = stats.recycle_stats.ring_full;
 	rq_stats->pp_recycle_released_ref = stats.recycle_stats.released_refcnt;
 }
+#else
+static void mlx5e_stats_update_stats_rq_page_pool(struct mlx5e_channel *c)
+{
+}
+#endif
 
 static MLX5E_DECLARE_STATS_GRP_OP_UPDATE_STATS(sw)
 {
@@ -530,13 +599,17 @@ static MLX5E_DECLARE_STATS_GRP_OP_UPDATE
 		int j;
 
 		mlx5e_stats_grp_sw_update_stats_rq_stats(s, &channel_stats->rq);
+#ifdef HAVE_XDP_SUPPORT
 		mlx5e_stats_grp_sw_update_stats_xdpsq(s, &channel_stats->rq_xdpsq);
 		mlx5e_stats_grp_sw_update_stats_ch_stats(s, &channel_stats->ch);
 		/* xdp redirect */
 		mlx5e_stats_grp_sw_update_stats_xdp_red(s, &channel_stats->xdpsq);
+#endif
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 		/* AF_XDP zero-copy */
 		mlx5e_stats_grp_sw_update_stats_xskrq(s, &channel_stats->xskrq);
 		mlx5e_stats_grp_sw_update_stats_xsksq(s, &channel_stats->xsksq);
+#endif
 
 		for (j = 0; j < priv->max_opened_tc; j++) {
 			mlx5e_stats_grp_sw_update_stats_sq(s, &channel_stats->sq[j]);
@@ -901,6 +974,7 @@ static MLX5E_DECLARE_STATS_GRP_OP_UPDATE
 		MLX5_BYTE_OFF(ppcnt_reg,		\
 			      counter_set.set.c##_high)))
 
+#if defined(HAVE_GET_PAUSE_STATS) || defined(HAVE_NDO_ETH_PHY_STATS)
 static int mlx5e_stats_get_ieee(struct mlx5_core_dev *mdev,
 				u32 *ppcnt_ieee_802_3)
 {
@@ -915,7 +989,9 @@ static int mlx5e_stats_get_ieee(struct m
 	return mlx5_core_access_reg(mdev, in, sz, ppcnt_ieee_802_3,
 				    sz, MLX5_REG_PPCNT, 0, 0);
 }
+#endif
 
+#ifdef HAVE_GET_PAUSE_STATS
 void mlx5e_stats_pause_get(struct mlx5e_priv *priv,
 			   struct ethtool_pause_stats *pause_stats)
 {
@@ -934,7 +1010,9 @@ void mlx5e_stats_pause_get(struct mlx5e_
 				      eth_802_3_cntrs_grp_data_layout,
 				      a_pause_mac_ctrl_frames_received);
 }
+#endif
 
+#ifdef HAVE_NDO_ETH_PHY_STATS
 void mlx5e_stats_eth_phy_get(struct mlx5e_priv *priv,
 			     struct ethtool_eth_phy_stats *phy_stats)
 {
@@ -1001,6 +1079,7 @@ void mlx5e_stats_eth_ctrl_get(struct mlx
 				      eth_802_3_cntrs_grp_data_layout,
 				      a_unsupported_opcodes_received);
 }
+#endif
 
 #define PPORT_2863_OFF(c) \
 	MLX5_BYTE_OFF(ppcnt_reg, \
@@ -1166,6 +1245,7 @@ void mlx5e_stats_rmon_get(struct mlx5e_p
 	*ranges = mlx5e_rmon_ranges;
 }
 
+#ifdef HAVE_NDO_STATS_TS_GET
 void mlx5e_stats_ts_get(struct mlx5e_priv *priv,
 			struct ethtool_ts_stats *ts_stats)
 {
@@ -1214,6 +1294,7 @@ void mlx5e_stats_ts_get(struct mlx5e_pri
 out:
 	mutex_unlock(&priv->state_lock);
 }
+#endif
 
 #define PPORT_PHY_LAYER_OFF(c) \
 	MLX5_BYTE_OFF(ppcnt_reg, \
@@ -1369,6 +1450,7 @@ static MLX5E_DECLARE_STATS_GRP_OP_UPDATE
 	}
 }
 
+#ifdef HAVE_NDO_LINK_EXT_STATS
 void mlx5e_get_link_ext_stats(struct net_device *dev,
 			      struct ethtool_link_ext_stats *stats)
 {
@@ -1385,7 +1467,9 @@ void mlx5e_get_link_ext_stats(struct net
 	stats->link_down_events = MLX5_GET(ppcnt_reg, out,
 					   counter_set.phys_layer_cntrs.link_down_events);
 }
+#endif
 
+#ifdef HAVE_NDO_GET_FEC_STATS
 static int fec_num_lanes(struct mlx5_core_dev *dev)
 {
 	u32 out[MLX5_ST_SZ_DW(pmlp_reg)] = {};
@@ -1502,6 +1586,7 @@ void mlx5e_stats_fec_get(struct mlx5e_pr
 	fec_set_corrected_bits_total(priv, fec_stats);
 	fec_set_block_stats(priv, fec_stats);
 }
+#endif
 
 #define PPORT_ETH_EXT_OFF(c) \
 	MLX5_BYTE_OFF(ppcnt_reg, \
@@ -2089,8 +2174,10 @@ static const struct counter_desc rq_stat
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, csum_unnecessary) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, csum_unnecessary_inner) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, csum_none) },
+#ifdef HAVE_XDP_SUPPORT
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, xdp_drop) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, xdp_redirect) },
+#endif
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, lro_packets) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, lro_bytes) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, gro_packets) },
@@ -2110,6 +2197,16 @@ static const struct counter_desc rq_stat
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, buff_alloc_err) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, cqe_compress_blks) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, cqe_compress_pkts) },
+#if !defined(HAVE_PAGE_POOL_DEFRAG_PAGE) && !defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
+	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, cache_reuse) },
+	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, cache_full) },
+	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, cache_empty) },
+	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, cache_busy) },
+	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, cache_ext) },
+	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, cache_rdc) },
+	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, cache_alloc) },
+	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, cache_waive) },
+#endif
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, congst_umr) },
 #ifdef CONFIG_MLX5_EN_ARFS
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, arfs_add) },
@@ -2119,6 +2216,7 @@ static const struct counter_desc rq_stat
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, arfs_err) },
 #endif
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, recover) },
+#ifdef CONFIG_PAGE_POOL_STATS
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, pp_alloc_fast) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, pp_alloc_slow) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, pp_alloc_slow_high_order) },
@@ -2130,6 +2228,7 @@ static const struct counter_desc rq_stat
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, pp_recycle_ring) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, pp_recycle_ring_full) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, pp_recycle_released_ref) },
+#endif
 #ifdef CONFIG_MLX5_EN_TLS
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, tls_decrypted_packets) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, tls_decrypted_bytes) },
@@ -2175,10 +2274,15 @@ static const struct counter_desc sq_stat
 	{ MLX5E_DECLARE_TX_STAT(struct mlx5e_sq_stats, xmit_more) },
 	{ MLX5E_DECLARE_TX_STAT(struct mlx5e_sq_stats, recover) },
 	{ MLX5E_DECLARE_TX_STAT(struct mlx5e_sq_stats, cqes) },
+#ifdef HAVE_BASECODE_EXTRAS
+	{ MLX5E_DECLARE_TX_STAT(struct mlx5e_sq_stats, cqe_compress_blks) },
+	{ MLX5E_DECLARE_TX_STAT(struct mlx5e_sq_stats, cqe_compress_pkts) },
+#endif
 	{ MLX5E_DECLARE_TX_STAT(struct mlx5e_sq_stats, wake) },
 	{ MLX5E_DECLARE_TX_STAT(struct mlx5e_sq_stats, cqe_err) },
 };
 
+#ifdef HAVE_XDP_SUPPORT
 static const struct counter_desc rq_xdpsq_stats_desc[] = {
 	{ MLX5E_DECLARE_RQ_XDPSQ_STAT(struct mlx5e_xdpsq_stats, xmit) },
 	{ MLX5E_DECLARE_RQ_XDPSQ_STAT(struct mlx5e_xdpsq_stats, mpwqe) },
@@ -2198,7 +2302,9 @@ static const struct counter_desc xdpsq_s
 	{ MLX5E_DECLARE_XDPSQ_STAT(struct mlx5e_xdpsq_stats, err) },
 	{ MLX5E_DECLARE_XDPSQ_STAT(struct mlx5e_xdpsq_stats, cqes) },
 };
+#endif
 
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 static const struct counter_desc xskrq_stats_desc[] = {
 	{ MLX5E_DECLARE_XSKRQ_STAT(struct mlx5e_rq_stats, packets) },
 	{ MLX5E_DECLARE_XSKRQ_STAT(struct mlx5e_rq_stats, bytes) },
@@ -2228,6 +2334,7 @@ static const struct counter_desc xsksq_s
 	{ MLX5E_DECLARE_XSKSQ_STAT(struct mlx5e_xdpsq_stats, err) },
 	{ MLX5E_DECLARE_XSKSQ_STAT(struct mlx5e_xdpsq_stats, cqes) },
 };
+#endif
 
 static const struct counter_desc ch_stats_desc[] = {
 	{ MLX5E_DECLARE_CH_STAT(struct mlx5e_ch_stats, events) },
@@ -2280,8 +2387,10 @@ static const struct counter_desc ptp_rq_
 	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, csum_unnecessary) },
 	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, csum_unnecessary_inner) },
 	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, csum_none) },
+#ifdef HAVE_XDP_SUPPORT
 	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, xdp_drop) },
 	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, xdp_redirect) },
+#endif
 	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, lro_packets) },
 	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, lro_bytes) },
 	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, ecn_mark) },
@@ -2293,6 +2402,13 @@ static const struct counter_desc ptp_rq_
 	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, buff_alloc_err) },
 	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, cqe_compress_blks) },
 	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, cqe_compress_pkts) },
+#if !defined(HAVE_PAGE_POOL_DEFRAG_PAGE) && !defined(HAVE_PAGE_POOL_PUT_UNREFED_PAGE)
+	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, cache_reuse) },
+	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, cache_full) },
+	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, cache_empty) },
+	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, cache_busy) },
+	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, cache_waive) },
+#endif
 	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, congst_umr) },
 	{ MLX5E_DECLARE_PTP_RQ_STAT(struct mlx5e_rq_stats, recover) },
 };
@@ -2334,10 +2450,14 @@ static const struct counter_desc qos_sq_
 
 #define NUM_RQ_STATS			ARRAY_SIZE(rq_stats_desc)
 #define NUM_SQ_STATS			ARRAY_SIZE(sq_stats_desc)
-#define NUM_XDPSQ_STATS			ARRAY_SIZE(xdpsq_stats_desc)
-#define NUM_RQ_XDPSQ_STATS		ARRAY_SIZE(rq_xdpsq_stats_desc)
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 #define NUM_XSKRQ_STATS			ARRAY_SIZE(xskrq_stats_desc)
 #define NUM_XSKSQ_STATS			ARRAY_SIZE(xsksq_stats_desc)
+#endif
+#ifdef HAVE_XDP_SUPPORT
+#define NUM_XDPSQ_STATS                 ARRAY_SIZE(xdpsq_stats_desc)
+#define NUM_RQ_XDPSQ_STATS              ARRAY_SIZE(rq_xdpsq_stats_desc)
+#endif
 #define NUM_CH_STATS			ARRAY_SIZE(ch_stats_desc)
 #define NUM_PTP_SQ_STATS		ARRAY_SIZE(ptp_sq_stats_desc)
 #define NUM_PTP_CH_STATS		ARRAY_SIZE(ptp_ch_stats_desc)
@@ -2475,16 +2595,23 @@ static MLX5E_DECLARE_STATS_GRP_OP_NUM_ST
 
 	return (NUM_RQ_STATS * max_nch) +
 	       (NUM_CH_STATS * max_nch) +
-	       (NUM_SQ_STATS * max_nch * priv->max_opened_tc) +
-	       (NUM_RQ_XDPSQ_STATS * max_nch) +
-	       (NUM_XDPSQ_STATS * max_nch) +
-	       (NUM_XSKRQ_STATS * max_nch * priv->xsk.ever_used) +
-	       (NUM_XSKSQ_STATS * max_nch * priv->xsk.ever_used);
+	       (NUM_SQ_STATS * max_nch * priv->max_opened_tc)
+#ifdef HAVE_XDP_SUPPORT
+	       + (NUM_RQ_XDPSQ_STATS * max_nch)
+	       + (NUM_XDPSQ_STATS * max_nch)
+#endif
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+	       + (NUM_XSKRQ_STATS * max_nch * priv->xsk.ever_used)
+	       + (NUM_XSKSQ_STATS * max_nch * priv->xsk.ever_used)
+#endif
+	       ;
 }
 
 static MLX5E_DECLARE_STATS_GRP_OP_FILL_STRS(channels)
 {
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	bool is_xsk = priv->xsk.ever_used;
+#endif
 	int max_nch = priv->stats_nch;
 	int i, j, tc;
 
@@ -2495,10 +2622,14 @@ static MLX5E_DECLARE_STATS_GRP_OP_FILL_S
 	for (i = 0; i < max_nch; i++) {
 		for (j = 0; j < NUM_RQ_STATS; j++)
 			ethtool_sprintf(data, rq_stats_desc[j].format, i);
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 		for (j = 0; j < NUM_XSKRQ_STATS * is_xsk; j++)
 			ethtool_sprintf(data, xskrq_stats_desc[j].format, i);
+#endif
+#ifdef HAVE_XDP_SUPPORT
 		for (j = 0; j < NUM_RQ_XDPSQ_STATS; j++)
 			ethtool_sprintf(data, rq_xdpsq_stats_desc[j].format, i);
+#endif
 	}
 
 	for (tc = 0; tc < priv->max_opened_tc; tc++)
@@ -2508,16 +2639,22 @@ static MLX5E_DECLARE_STATS_GRP_OP_FILL_S
 						i + tc * max_nch);
 
 	for (i = 0; i < max_nch; i++) {
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 		for (j = 0; j < NUM_XSKSQ_STATS * is_xsk; j++)
 			ethtool_sprintf(data, xsksq_stats_desc[j].format, i);
+#endif
+#ifdef HAVE_XDP_SUPPORT
 		for (j = 0; j < NUM_XDPSQ_STATS; j++)
 			ethtool_sprintf(data, xdpsq_stats_desc[j].format, i);
+#endif
 	}
 }
 
 static MLX5E_DECLARE_STATS_GRP_OP_FILL_STATS(channels)
 {
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	bool is_xsk = priv->xsk.ever_used;
+#endif
 	int max_nch = priv->stats_nch;
 	int i, j, tc;
 
@@ -2534,16 +2671,20 @@ static MLX5E_DECLARE_STATS_GRP_OP_FILL_S
 				data, MLX5E_READ_CTR64_CPU(
 					      &priv->channel_stats[i]->rq,
 					      rq_stats_desc, j));
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 		for (j = 0; j < NUM_XSKRQ_STATS * is_xsk; j++)
 			mlx5e_ethtool_put_stat(
 				data, MLX5E_READ_CTR64_CPU(
 					      &priv->channel_stats[i]->xskrq,
 					      xskrq_stats_desc, j));
+#endif
+#ifdef HAVE_XDP_SUPPORT
 		for (j = 0; j < NUM_RQ_XDPSQ_STATS; j++)
 			mlx5e_ethtool_put_stat(
 				data, MLX5E_READ_CTR64_CPU(
 					      &priv->channel_stats[i]->rq_xdpsq,
 					      rq_xdpsq_stats_desc, j));
+#endif
 	}
 
 	for (tc = 0; tc < priv->max_opened_tc; tc++)
@@ -2556,16 +2697,20 @@ static MLX5E_DECLARE_STATS_GRP_OP_FILL_S
 						sq_stats_desc, j));
 
 	for (i = 0; i < max_nch; i++) {
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 		for (j = 0; j < NUM_XSKSQ_STATS * is_xsk; j++)
 			mlx5e_ethtool_put_stat(
 				data, MLX5E_READ_CTR64_CPU(
 					      &priv->channel_stats[i]->xsksq,
 					      xsksq_stats_desc, j));
+#endif
+#ifdef HAVE_XDP_SUPPORT
 		for (j = 0; j < NUM_XDPSQ_STATS; j++)
 			mlx5e_ethtool_put_stat(
 				data, MLX5E_READ_CTR64_CPU(
 					      &priv->channel_stats[i]->xdpsq,
 					      xdpsq_stats_desc, j));
+#endif
 	}
 }
 
