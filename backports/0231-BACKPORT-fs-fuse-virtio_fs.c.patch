From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: fs/fuse/virtio_fs.c

---
 fs/fuse/virtio_fs.c | 90 +++++++++++++++++++++++++++++++++++++++++++--
 1 file changed, 87 insertions(+), 3 deletions(-)

--- a/fs/fuse/virtio_fs.c
+++ b/fs/fuse/virtio_fs.c
@@ -9,7 +9,9 @@
 #include <linux/pci.h>
 #include <linux/interrupt.h>
 #include <linux/group_cpus.h>
+#ifdef HAVE_LINUX_PFN_T_H
 #include <linux/pfn_t.h>
+#endif
 #include <linux/memremap.h>
 #include <linux/module.h>
 #include <linux/virtio.h>
@@ -940,7 +942,9 @@ static void virtio_fs_end_notify(struct
 		virtio_fs_notify_send_ack(fs, fs_req->req.out.h.unique, err);
 		break;
 	case FUSE_NOTIFY_RETRIEVE:
+#ifdef HAVE_FUSE_NOTIFY_RESEND
 	case FUSE_NOTIFY_RESEND:
+#endif
 	case FUSE_NOTIFY_POLL:
 	default:
 		break;
@@ -1323,6 +1327,9 @@ static void virtio_fs_map_queues(struct
 {
 	const struct cpumask *mask, *masks;
 	unsigned int q, cpu;
+#ifdef HAVE_GROUP_CPUS_EVENLY_NUMMASKS
+	unsigned int nr_masks;
+#endif
 
 	/* First attempt to map using existing transport layer affinities
 	 * e.g. PCIe MSI-X
@@ -1342,7 +1349,13 @@ static void virtio_fs_map_queues(struct
 	return;
 fallback:
 	/* Attempt to map evenly in groups over the CPUs */
+#ifdef HAVE_GROUP_CPUS_EVENLY_NUMMASKS
+	/* Newer kernels (v6.17-rc1+): function returns actual number of masks */
+	masks = group_cpus_evenly(fs->num_request_queues, &nr_masks);
+#else
+	/* Older kernels: function returns masks for all requested groups */
 	masks = group_cpus_evenly(fs->num_request_queues);
+#endif
 	/* If even this fails we default to all CPUs use first request queue */
 	if (!masks) {
 		for_each_possible_cpu(cpu)
@@ -1351,7 +1364,13 @@ fallback:
 	}
 
 	for (q = 0; q < fs->num_request_queues; q++) {
+#ifdef HAVE_GROUP_CPUS_EVENLY_NUMMASKS
+		/* Newer kernels (v6.17-rc1+): use modulo to avoid out-of-bounds */
+		for_each_cpu(cpu, &masks[q % nr_masks])
+#else
+		/* Older kernels: direct indexing is safe */
 		for_each_cpu(cpu, &masks[q])
+#endif
 			fs->mq_map[cpu] = q + VQ_REQUEST;
 	}
 	kfree(masks);
@@ -1407,7 +1426,12 @@ static void virtio_fs_cleanup_vqs(struct
 static int virtio_fs_setup_vqs(struct virtio_device *vdev,
 			       struct virtio_fs *fs, bool alloc_reqs)
 {
+#ifdef HAVE_VIRTQUEUE_INFO
 	struct virtqueue_info *vqs_info;
+#else
+	vq_callback_t **callbacks;
+	const char **names;
+#endif
 	unsigned int info_idx = 0;
 	struct virtqueue **vqs;
 	unsigned int vq_nvqs;
@@ -1416,33 +1440,59 @@ static int virtio_fs_setup_vqs(struct vi
 
 	vq_nvqs = 1 + fs->notify_enabled + fs->num_request_queues;
 	vqs = kmalloc_array(vq_nvqs, sizeof(vqs[VQ_HIPRIO]), GFP_KERNEL);
+#ifdef HAVE_VIRTQUEUE_INFO
 	vqs_info = kcalloc(vq_nvqs, sizeof(*vqs_info), GFP_KERNEL);
 	if (!vqs || !vqs_info) {
+#else
+	callbacks = kmalloc_array(vq_nvqs, sizeof(callbacks[VQ_HIPRIO]),
+				  GFP_KERNEL);
+	names = kmalloc_array(vq_nvqs, sizeof(names[VQ_HIPRIO]), GFP_KERNEL);
+	if (!vqs || !callbacks || !names) {	
+#endif
 		ret = -ENOMEM;
 		goto out;
 	}
 
 	/* Initialize the hiprio/forget request virtqueue */
+#ifdef HAVE_VIRTQUEUE_INFO
 	vqs_info[info_idx].callback = virtio_fs_vq_done;
 	vqs_info[info_idx].name = fs->vqs[VQ_HIPRIO].name;
+#else
+	callbacks[info_idx] = virtio_fs_vq_done;
+	names[info_idx] = fs->vqs[VQ_HIPRIO].name;
+#endif
 	info_idx++;
 
 	if (fs->notify_enabled) {
 		/* Initialize the notify request virtqueue */
+#ifdef HAVE_VIRTQUEUE_INFO
 		vqs_info[info_idx].callback = virtio_fs_vq_done;
 		vqs_info[info_idx].name = fs->vqs[VQ_NOTIFY].name;
+#else
+		callbacks[info_idx] = virtio_fs_vq_done;
+		names[info_idx] = fs->vqs[VQ_NOTIFY].name;
+#endif
 		info_idx++;
 	}
 
 	/* Initialize the requests virtqueues */
 	for (i = VQ_REQUEST; i < fs->nvqs; i++) {
+#ifdef HAVE_VIRTQUEUE_INFO
 		vqs_info[info_idx].callback = virtio_fs_vq_done;
 		vqs_info[info_idx].name = fs->vqs[i].name;
+#else
+		callbacks[info_idx] = virtio_fs_vq_done;
+		names[info_idx] = fs->vqs[i].name;
+#endif
 		info_idx++;
 	}
 
 	/* Leaving irq descriptor NULL will allow for dynamic remapping */
+#ifdef HAVE_VIRTQUEUE_INFO
 	ret = virtio_find_vqs(vdev, vq_nvqs, vqs, vqs_info, NULL);
+#else
+	ret = virtio_find_vqs(vdev, vq_nvqs, vqs, callbacks, names, NULL);
+#endif
 	if (ret < 0)
 		goto out;
 
@@ -1461,7 +1511,12 @@ static int virtio_fs_setup_vqs(struct vi
 			goto out_del_vqs;
 	}
 
+#ifdef HAVE_VIRTQUEUE_INFO
 	kfree(vqs_info);
+#else
+	kfree(names);
+	kfree(callbacks);
+#endif
 	kfree(vqs);
 	return 0;
 
@@ -1469,7 +1524,12 @@ out_del_vqs:
 	virtio_reset_device(vdev);
 	virtio_fs_cleanup_vqs(vdev);
 out:
+#ifdef HAVE_VIRTQUEUE_INFO
 	kfree(vqs_info);
+#else
+	kfree(names);
+	kfree(callbacks);
+#endif
 	kfree(vqs);
 	return ret;
 }
@@ -1547,7 +1607,11 @@ err:
  */
 static long virtio_fs_direct_access(struct dax_device *dax_dev, pgoff_t pgoff,
 				    long nr_pages, enum dax_access_mode mode,
+#ifndef HAVE_LINUX_PFN_T_H
+				    void **kaddr, unsigned long *pfn)
+#else
 				    void **kaddr, pfn_t *pfn)
+#endif
 {
 	struct virtio_fs *fs = dax_get_private(dax_dev);
 	phys_addr_t offset = PFN_PHYS(pgoff);
@@ -1555,9 +1619,16 @@ static long virtio_fs_direct_access(stru
 
 	if (kaddr)
 		*kaddr = fs->window_kaddr + offset;
-	if (pfn)
+	if (pfn) {
+#ifndef HAVE_LINUX_PFN_T_H
+		/* Newer kernels (v6.17-rc1+): use unsigned long pfn */
+		*pfn = PHYS_PFN(fs->window_phys_addr + offset);
+#else
+		/* Older kernels: use pfn_t with flags */
 		*pfn = phys_to_pfn_t(fs->window_phys_addr + offset,
-					PFN_DEV | PFN_MAP);
+				     PFN_DEV | PFN_MAP);
+#endif
+	}
 	return nr_pages > max_nr_pages ? max_nr_pages : nr_pages;
 }
 
@@ -1764,6 +1835,7 @@ static int virtio_fs_restore(struct virt
 }
 #endif /* CONFIG_PM_SLEEP */
 
+#ifdef HAVE_VIRTIO_DEVICE_RESET_PREPARE
 static int virtio_fs_reset_prepare(struct virtio_device *vdev)
 {
 	struct virtio_fs *fs = vdev->priv;
@@ -1802,6 +1874,7 @@ static int virtio_fs_reset_done(struct v
 
 	return 0;
 }
+#endif
 
 static const struct virtio_device_id id_table[] = {
 	{ VIRTIO_ID_FS, VIRTIO_DEV_ANY_ID },
@@ -1823,8 +1896,10 @@ static struct virtio_driver virtio_fs_dr
 	.freeze			= virtio_fs_freeze,
 	.restore		= virtio_fs_restore,
 #endif
+#ifdef HAVE_VIRTIO_DEVICE_RESET_PREPARE
 	.reset_prepare		= virtio_fs_reset_prepare,
 	.reset_done		= virtio_fs_reset_done,
+#endif
 };
 
 #define FUSE_REQ_ID_STEP (1ULL << 1)
@@ -2519,7 +2594,9 @@ static void virtio_fs_conn_resend(struct
 		list_for_each_entry_safe(fs_req, next, &fsvq->processing, entry) {
 			__virtio_fs_get_request(fs_req);
 			/* mark the request as resend request */
+#ifdef HAVE_FUSE_NOTIFY_RESEND
 			fs_req->req.in.h.unique |= FUSE_UNIQUE_RESEND;
+#endif
 			list_move_tail(&fs_req->entry, &fsvq->queued_reqs);
 		}
 
@@ -2735,7 +2812,14 @@ static int __init virtio_fs_init(void)
 {
 	int ret;
 
-	pr_info("virtio-fs: Loading NVIDIA-virtiofs +mq +lockless +nvq +flr +gds\n");
+	pr_info("virtio-fs: Loading NVIDIA-virtiofs +mq +lockless +nvq"
+#ifdef HAVE_VIRTIO_DEVICE_RESET_PREPARE
+		" +flr"
+#endif
+#ifdef HAVE_ITER_ALLOW_P2PDMA
+		" +gds"
+#endif
+		"\n");
 
 	fuse_inode_cachep = kmem_cache_create("nvidiavfs_inode",
                                       sizeof(struct fuse_inode), 0,
