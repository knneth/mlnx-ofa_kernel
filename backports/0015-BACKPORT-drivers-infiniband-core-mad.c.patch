From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/infiniband/core/mad.c

---
 drivers/infiniband/core/mad.c | 52 +++++++++++++++++++++++++++++------
 1 file changed, 44 insertions(+), 8 deletions(-)

--- a/drivers/infiniband/core/mad.c
+++ b/drivers/infiniband/core/mad.c
@@ -54,6 +54,7 @@
 #include "opa_smi.h"
 #include "agent.h"
 
+#ifndef MLX_DISABLE_TRACEPOINTS
 #define CREATE_TRACE_POINTS
 #include <trace/events/ib_mad.h>
 
@@ -74,6 +75,7 @@ static void create_mad_addr_info(struct
 	entry->dlid = rdma_ah_get_dlid(&attr);
 }
 #endif
+#endif
 
 static int mad_sendq_size = IB_MAD_QP_SEND_SIZE;
 static int mad_recvq_size = IB_MAD_QP_RECV_SIZE;
@@ -517,7 +519,9 @@ struct ib_mad_agent *ib_register_mad_age
 	}
 	spin_unlock_irq(&port_priv->reg_lock);
 
+#ifndef MLX_DISABLE_TRACEPOINTS
 	trace_ib_mad_create_agent(mad_agent_priv);
+#endif
 	return &mad_agent_priv->agent;
 error6:
 	spin_unlock_irq(&port_priv->reg_lock);
@@ -543,9 +547,11 @@ static void unregister_mad_agent(struct
 {
 	struct ib_mad_port_private *port_priv;
 
+#ifndef MLX_DISABLE_TRACEPOINTS
 	/* Note that we could still be handling received MADs */
 	trace_ib_mad_unregister_agent(mad_agent_priv);
 
+#endif
 	/*
 	 * Canceling all sends results in dropping received response
 	 * MADs, preventing us from queuing additional work
@@ -683,7 +689,9 @@ static int handle_outgoing_dr_smp(struct
 	if (opa && smp->class_version == OPA_SM_CLASS_VERSION) {
 		u32 opa_drslid;
 
+#ifndef MLX_DISABLE_TRACEPOINTS
 		trace_ib_mad_handle_out_opa_smi(opa_smp);
+#endif
 
 		if ((opa_get_smp_direction(opa_smp)
 		     ? opa_smp->route.dr.dr_dlid : opa_smp->route.dr.dr_slid) ==
@@ -710,8 +718,9 @@ static int handle_outgoing_dr_smp(struct
 		    opa_smi_check_local_returning_smp(opa_smp, device) == IB_SMI_DISCARD)
 			goto out;
 	} else {
+#ifndef MLX_DISABLE_TRACEPOINTS
 		trace_ib_mad_handle_out_ib_smi(smp);
-
+#endif
 		if ((ib_get_smp_direction(smp) ? smp->dr_dlid : smp->dr_slid) ==
 		     IB_LID_PERMISSIVE &&
 		     smi_handle_dr_smp_send(smp, rdma_cap_ib_switch(device), port_num) ==
@@ -1087,7 +1096,9 @@ int ib_send_mad(struct ib_mad_send_wr_pr
 
 	spin_lock_irqsave(&qp_info->send_queue.lock, flags);
 	if (qp_info->send_queue.count < qp_info->send_queue.max_active) {
+#ifndef MLX_DISABLE_TRACEPOINTS
 		trace_ib_mad_ib_send_mad(mad_send_wr, qp_info);
+#endif
 		ret = ib_post_send(mad_agent->qp, &mad_send_wr->send_wr.wr,
 				   NULL);
 		list = &qp_info->send_queue.list;
@@ -1877,7 +1888,6 @@ out:
 		deref_mad_agent(mad_agent);
 		mad_agent = NULL;
 	}
-
 	return mad_agent;
 }
 
@@ -2188,7 +2198,9 @@ static enum smi_action handle_ib_smi(con
 	enum smi_forward_action retsmi;
 	struct ib_smp *smp = (struct ib_smp *)recv->mad;
 
+#ifndef MLX_DISABLE_TRACEPOINTS
 	trace_ib_mad_handle_ib_smi(smp);
+#endif
 
 	if (smi_handle_dr_smp_recv(smp,
 				   rdma_cap_ib_switch(port_priv->device),
@@ -2274,9 +2286,9 @@ handle_opa_smi(struct ib_mad_port_privat
 {
 	enum smi_forward_action retsmi;
 	struct opa_smp *smp = (struct opa_smp *)recv->mad;
-
+#ifndef MLX_DISABLE_TRACEPOINTS
 	trace_ib_mad_handle_opa_smi(smp);
-
+#endif
 	if (opa_smi_handle_dr_smp_recv(smp,
 				   rdma_cap_ib_switch(port_priv->device),
 				   port_num,
@@ -2397,10 +2409,10 @@ static void ib_mad_recv_done(struct ib_c
 	/* Validate MAD */
 	if (!validate_mad((const struct ib_mad_hdr *)recv->mad, qp_info, opa))
 		goto out;
-
+#ifndef MLX_DISABLE_TRACEPOINTS
 	trace_ib_mad_recv_done_handler(qp_info, wc,
 				       (struct ib_mad_hdr *)recv->mad);
-
+#endif
 	mad_size = recv->mad_size;
 	response = alloc_mad_private(mad_size, GFP_KERNEL);
 	if (!response)
@@ -2447,7 +2459,9 @@ static void ib_mad_recv_done(struct ib_c
 
 	mad_agent = find_mad_agent(port_priv, (const struct ib_mad_hdr *)recv->mad);
 	if (mad_agent) {
+#ifndef MLX_DISABLE_TRACEPOINTS
 		trace_ib_mad_recv_done_agent(mad_agent);
+#endif
 		ib_mad_complete_recv(mad_agent, &recv->header.recv_wc);
 		/*
 		 * recv is freed up in error cases in ib_mad_complete_recv
@@ -2589,10 +2603,10 @@ static void ib_mad_send_done(struct ib_c
 				   mad_list);
 	send_queue = mad_list->mad_queue;
 	qp_info = send_queue->qp_info;
-
+#ifndef MLX_DISABLE_TRACEPOINTS
 	trace_ib_mad_send_done_agent(mad_send_wr->mad_agent_priv);
 	trace_ib_mad_send_done_handler(mad_send_wr, wc);
-
+#endif
 retry:
 	ib_dma_unmap_single(mad_send_wr->send_buf.mad_agent->device,
 			    mad_send_wr->header_mapping,
@@ -2621,7 +2635,9 @@ retry:
 	ib_mad_complete_send_wr(mad_send_wr, &mad_send_wc);
 
 	if (queued_send_wr) {
+#ifndef MLX_DISABLE_TRACEPOINTS
 		trace_ib_mad_send_done_resend(queued_send_wr, qp_info);
+#endif
 		ret = ib_post_send(qp_info->qp, &queued_send_wr->send_wr.wr,
 				   NULL);
 		if (ret) {
@@ -2669,7 +2685,9 @@ static bool ib_mad_send_error(struct ib_
 		if (mad_send_wr->retry) {
 			/* Repost send */
 			mad_send_wr->retry = 0;
+#ifndef MLX_DISABLE_TRACEPOINTS
 			trace_ib_mad_error_handler(mad_send_wr, qp_info);
+#endif
 			ret = ib_post_send(qp_info->qp, &mad_send_wr->send_wr.wr,
 					   NULL);
 			if (!ret)
@@ -3377,7 +3395,11 @@ static ssize_t sa_cc_attr_store(struct k
 	return sa->store(cc_obj, buf, size);
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
 static const struct sysfs_ops sa_cc_sysfs_ops = {
+#else
+static struct sysfs_ops sa_cc_sysfs_ops = {
+#endif
 	.show = sa_cc_attr_show,
 	.store = sa_cc_attr_store,
 };
@@ -3398,11 +3420,17 @@ static struct attribute *sa_cc_default_a
 	NULL
 };
 
+#ifdef HAVE_KOBJ_TYPE_DEFAULT_GROUPS
 ATTRIBUTE_GROUPS(sa_cc_default);
+#endif
 
 static struct kobj_type sa_cc_type = {
 	.sysfs_ops = &sa_cc_sysfs_ops,
+#ifdef HAVE_KOBJ_TYPE_DEFAULT_GROUPS
 	.default_groups = sa_cc_default_groups
+#else
+	.default_attrs = sa_cc_default_attrs
+#endif
 };
 
 static void cleanup_sa_cc_sysfs_ports(struct sa_cc_data *cc_obj)
